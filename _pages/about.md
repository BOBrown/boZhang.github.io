---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<div style='font-size:14pt; text-align:justify; font-family:Georgia; margin-top: 50pt'>
  <div style='width: 95%; vertical-align: middle; margin-left: 3%'>
Bo Zhang received the Ph.D. degree in electronic engineering from the School of Information Science and Technology, Fudan University. He is currently a Research Scientist in Shanghai AI Laboratory. His work has led to many awards, including Shanghai Rising Star under Grant No. 23QD1401000, awarded by the Shanghai Municipal Commission of Science and Technology, the National Scholarship 2021 China Award, the 2019 Excellent Doctoral Scholarship of Fudan University Award, and various awards from VALSE China and Shanghai Government. His research outcomes have some impacts on industrial applications like airport checkpoint security recognition, domain adaptive face recognition, and localization of concealed dangerous objects. 
  </div>
</div>

<div style='font-size:14pt; text-align:justify; font-family:Georgia; margin-top: 50pt'>
  <div style='width: 95%; vertical-align: middle; margin-left: 3%'>
He has published 30+ papers in top-tier international conferences and journals such as CVPR, NeurIPS, ICLR, ICML, ACL, T-PAMI, TIP, T-MM, and IJCV. He also serves as a reviewer for several prestigious academic conferences and journals, including CVPR, ECCV, ICCV, ICLR, and ICML. He led the development of the 3DTrans general scene representation open-source project, which won the Waymo Challenge international competition and accumulated over 1.5k stars. Furthermore, he is committed to exploring the fundamental nature of long-chain reasoning in large models and aims to develop innovator-level agents through reinforcement learning methods and reflection mechanism.
  </div>
</div>

<div style='font-size:14pt; text-align:justify; font-family:Georgia; margin-top: 50pt'>
  <div style='width: 95%; vertical-align: middle; margin-left: 3%'>
  üöÄ Join Shanghai AI Lab's Elite Team! <br>
  We're recruiting PhDs (2025 intake) & Researcher (March/June 2025 start) to pioneer LLM, Multi-Agent Optimization, and AutoGPT innovations.
  <br>
  üëâ Contact now with your CV + research vision: zhangbo@pjlab.org.cn & bo.zhangzx@gmail.com
  </div>
</div>

<div style='margin-top: 30pt'></div>

# üî• Highlighted Projects
- üê¨ <b>Dolphin</b>. (<font color="red">Auto-research framework</font> that achieves closed-loop LLM-driven framework to enhance the automation level of scientific research.) 
[[**Project**]](https://alpha-innovator.github.io/Dolphin-project-page)[[**Technical report**]](https://arxiv.org/abs/2501.03916)

- <b>MinerU</b> and <b>PDF-Extract-Kit</b>. (<font color="red">A popular open-source tool</font>, converting PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format.) 
[[**Project**]](https://github.com/opendatalab/PDF-Extract-Kit)[[**Technical report**]](https://arxiv.org/abs/2409.18839)

- <b>InternVL 1.5</b> and <b>InternVL 2</b>. (<font color="red">Rank 1st</font> among open-source VLM models on MMMU, DocVQA, ChartQA, and MathVista.) 
[[**Project**]](https://github.com/OpenGVLab/InternVL)[[**Technical report**]](https://arxiv.org/abs/2404.16821)

- <b>3DTrans (Work during the PhD period)</b>. (An Open-source Codebase for Continuous Learning towards Autonomous Driving Task, including Unsupervised Domain Adaptation (UDA), Active Domain Adaptation (ADA), Semi-Supervised Domain Adaptation (SSDA), and Multi-dateset Domain Fusion (MDF) tasks.) 
[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Technical report**]](https://arxiv.org/abs/2303.06880)

# üåé News
**2025:**
  - <p style='text-align:justify'><i>2025.02</i>: &nbsp; Three papers are accepted by <font color="red">CVPR-2025</font>: JiSAM, OmniDocBench, CDM.</p>
  - <p style='text-align:justify'><i>2025.01</i>: &nbsp; One of our papers has been accepted for publication in <font color="red">TPAMI</font>, another has been accepted by <font color="red">TGRS</font>.</p>
  - <p style='text-align:justify'><i>2025.01</i>: &nbsp; üéâüéâ Two papers accepted to <font color="red">ICLR 2025</font>: <a href="https://arxiv.org/abs/2412.11863">GeoX</a>, <a href="https://arxiv.org/abs/2406.08418">OmniCorpus</a></p>
  - <p style='text-align:justify'><i>2025.01</i>: &nbsp;üî•üî•üéâüéâ We propose a closed-loop open-ended auto-research framework named Dolphin, aiming to simulate and further extend the entire process of human scientific research. For more video demonstrations, please visit the project homepage: <a href="https://alpha-innovator.github.io/Dolphin-project-page/">Project Homepage</a></p>

**2024:**
  - <p style='text-align:justify'><i>2024.12</i>: &nbsp; Our research project, <a href="https://www.arxiv.org/abs/2412.11863/">GeoX</a>, has been officially open-sourced today. It is the first to explore formalized visual-language pre-training in enhancing geometric problem-solving abilities.</p>
  - <p style='text-align:justify'><i>2024.10</i>: &nbsp;üéâüéâ <font color="red">Grateful for the heartfelt recognition and thoughtful sharing of my research work</font> <a href="https://www.sohu.com/a/814145951_121124291/">Fudan_CYL</a> and <a href="https://mp.weixin.qq.com/s/KVAI0Q7spu6yot-6oQOewQ">Fudan_SIST</a> .</p>
  - <p style='text-align:justify'><i>2024.10</i>: &nbsp; The technical report for <a href="https://arxiv.org/abs/2409.18839/"><font color="red">MinerU</font></a>, an open-source solution for high-precision document content extraction, has been published. </p>
  - <p style='text-align:justify'><i>2024.09</i>: &nbsp;üéâüéâ Three papers accepted to <font color="red">NeurIPS 2024</font>: <a href="https://arxiv.org/abs/2410.09873/">AdaptiveDiffusion</a>, <a href="https://arxiv.org/abs/2411.05311/">ZOPP</a>, <a href="https://arxiv.org/abs/2405.15324/">LeapAD</a> </p>
  - <p style='text-align:justify'><i>2024.09</i>: &nbsp; Previous evaluation metrics for Formula and Table Recognition tasks, such as BLEU and Edit Distrance, exhibit limitations. <a href="https://github.com/opendatalab/UniMERNet/tree/main/cdm/">Our CDM</a> has been released to ensure the evaluation objectivity by designing an image-level rather than LaTex-level metric score for Formula and Table Recognition evaluation.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp;üéâüéâ Bo Zhang was invited to serve as a PC member of <font color="red">AAAI 2025</font>.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp; We open-sourced StructTable: Table Structural Extraction Model <a href="https://huggingface.co/U4R/StructTable-base/">Models</a> and <a href="https://github.com/UniModal4Reasoning/StructEqTable-Deploy/">StructEqTable-Deploy</a>. It is a open-source repository to support the structuring tasks of visual tables.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp;üéâüéâ We collaborated with the OpenDataLab team to open-source the <font color="red">PDF-Extract-Kit</font>. It can extract high-quality and structured content from PDFs and has gained <b>6K+</b> stars.</p>
  - <p style='text-align:justify'><i>2024.07</i>: &nbsp; One paper (Reg-TTA3D) is accepted by <font color="red">ECCV 2024</font>. We explore test-time adaptive 3d object detection for the first time.</p>
  - <p style='text-align:justify'><i>2024.05</i>: &nbsp;üéâüéâ Our paper entitled "Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm"  is accepted for publication in <font color="red">ICML 2024</font>.</p>
  - <p style='text-align:justify'><i>2024.03</i>: &nbsp; One paper is accepted by <font color="red">ACL 2024</font>. We propose All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.</p>
  - <p style='text-align:justify'><i>2024.02</i>: &nbsp; One paper (Once for Both) is accepted by <font color="red">CVPR-2024</font>. Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression.</p>
  - <p style='text-align:justify'><i>2024.01</i>: &nbsp; One paper (ReSimAD) is accepted by <font color="red">ICLR 2024</font>. We propose a zero-shot generalization framework by reconstructing mesh and simulating target point clouds.</p>
  - <p style='text-align:justify'><i>2024.01</i>: &nbsp; Two papers (<a href="https://ieeexplore.ieee.org/abstract/document/10516600/">IPNet</a> and <a href="https://ieeexplore.ieee.org/abstract/document/10360874/">MVNet</a>) are accepted by <font color="red">TCSVT</font>. </p>

**2023:**
  - <p style='text-align:justify'><i>2023.12</i>: &nbsp; We have released the <a href="https://github.com/UniModal4Reasoning/ChartVLM?tab=readme-ov-file">ChartX benchmark</a>, covering 18 chart types, 7 chart tasks, 22 disciplinary topics to evaluate the chart-related capabilities of the existing MLLMS.</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp; StructChart: our research on visual chart, has been released <a href="https://arxiv.org/abs/2309.11268">arXiv paper</a>, where we will release the SimChart9K dataset powered by LLM. By the proposed SimChart9K, we observe that StructChart continuously improves the chart perception performance as more simulated charts are used for pre-training.</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp; SPOT, showing a promising and <b>scalable</b> 3D pre-training on autonomous driving, has been released (See our paper for more details, <a href="https://arxiv.org/abs/2309.10527">arXiv paper</a>).</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp;üéâüéâ - One paper entitled ‚ÄúAD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset‚Äù is accepted by <font color="red">NeurIPS-2023</font>.</p>
  - <p style='text-align:justify'><i>2023.07</i>: &nbsp; One paper about cross-domain background-fouced alignment "Rethinking Cross-Domain Pedestrian Detection: A Background-Focused Distribution Alignment Framework for Instance-Free One-Stage Detectors" is accepted by <font color="red">TIP</font>.</p>
  - <p style='text-align:justify'><i>2023.07</i>: &nbsp; One paper entitled "SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification" is accepted by <font color="red">ACM MM-2023</font>.</p>
  - <p style='text-align:justify'><i>2023.04</i>: &nbsp; One paper entitled "Performance-aware Approximation of Global Channel Pruning for Multitask CNNs" is accepted for publication in <font color="red">T-PAMI</font>.</p>
  - <p style='text-align:justify'><i>2023.03</i>: &nbsp;üéâüéâ <b>Three papers</b> are accepted by <font color="red">CVPR-2023</font>: Uni3D, Bi3D, GDP.</p>
  - <p style='text-align:justify'><i>2023.02</i>: &nbsp; Bo Zhang started to work on exploring how to improve the problem-solving and reasoning ability of LLMs or VLMs for complicated modalities, including <b>Chart, Table, Geometry, Scientific Document</b>, by investigating foundation LLM models from the perspective of structured knowledge-rich data.</p>


<div style='margin-top: 30pt'></div>

# üìù Selected Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='images/MME-CoT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/abs/2502.09621)

Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, **<u>Bo Zhang</u>**, Chaoyou Fu, Peng Gao, Hongsheng Li


[[**Project**]](https://mmecot.github.io/)[[**Paper**]](https://arxiv.org/pdf/2502.09621)
- We introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs
- MME-CoT covers six domains: math science, OCR, logic, space-time, and general scenes. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='images/MME-CoT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training](https://arxiv.org/abs/2412.11863)

Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, **<u>Bo Zhang^(corr.)</u>**

[[**Project**]](https://github.com/Alpha-Innovator/GeoX)[[**Paper**]](https://arxiv.org/abs/2412.11863)
- Our study reveals the large potential of formalized visual-language pre-training in enhancing geometric problem-solving abilities. To enable the formalized pre-training, we propose GeoX, aiming to build geometric generalist models by modeling geometric tasks into a unified formulation.
- We propose a Generator-And-Sampler Transformer (GS-Former) to generate discriminative queries and eliminate uninformative representations from unevenly distributed geometric signals. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/omnicorups.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](https://arxiv.org/abs/2406.08418)

Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, **<u>Bo Zhang</u>**, Pinlong Cai, Licheng Wen, Xiangchao Yan, Zhenxiang Li, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Zhongying Tu, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai

[[**Project**]](https://github.com/OpenGVLab/OmniCorpus)[[**Paper**]](https://arxiv.org/abs/2406.08418)
- We filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion tet tokens.
- Our dataset is 15 times larger with high quality, features diverse sources (including English, non-English, and video-centric websites), and offers flexibility to adapt from an image-text interleaved format to pure text or image-text pairs. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SCIS</div><img src='images/internvl-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)

Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, **<u>Bo Zhang</u>**, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang

[[**Project**]](https://github.com/OpenGVLab/InternVL)[[**Paper**]](https://arxiv.org/abs/2404.16821)
- Propose InternVL 1.5 and InternVL 2. (<font color="red">Rank 1st</font> among open-source VLM models on MMMU, DocVQA, ChartQA, and MathVista.) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/adaptivediffusion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy](https://arxiv.org/abs/2410.09873)

Hancheng Ye, Jiakang Yuan, Renqiu Xia, Xiangchao Yan, Tao Chen, Junchi Yan, Botian Shi, **<u>Bo Zhang^(corr.)</u>**

[[**Project**]](https://jiakangyuan.github.io/AdaptiveDiffusion-project-page/)[[**Paper**]](https://arxiv.org/abs/2410.09873)
- Propose AdaptiveDiffusion to adaptively reduce the noise prediction steps during the denoising proces guided by the third-order latent difference. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/zopp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ZOPP: A Framework of Zero-shot Offboard Panoptic Perception for Autonomous Driving](https://arxiv.org/abs/2411.05311)

Tao Ma, Hongbin Zhou, Qiusheng Huang, Xuemeng Yang, Jianfei Guo, **<u>Bo Zhang</u>**, Min Dou, Yu Qiao, Botian Shi, Hongsheng Li

[[**Project**]]()[[**Paper**]](https://arxiv.org/abs/2411.05311)
- ZOPP integrates the powerful zero-shot recognition capabilities of vision foundation models and 3D representations derived from point clouds.
</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/leapad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving](https://arxiv.org/abs/2405.15324)

Jianbiao Mei, Yukai Ma, Xuemeng Yang, Licheng Wen, Xinyu Cai, Xin Li, Daocheng Fu, **<u>Bo Zhang</u>**, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yong Liu, Yu Qiao

[[**Project**]](https://leapad-2024.github.io/LeapAD/)[[**Paper**]](https://arxiv.org/abs/2405.15324)
- LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing.
</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/ctl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm](https://arxiv.org/abs/2402.03660)

Zhanpeng Zhou, Zijun Chen, Yilan Chen, **<u>Bo Zhang^(corr.)</u>**, Junchi Yan

[[**Project**]](https://github.com/zzp1012/Cross-Task-Linearity)[[**Paper**]](https://arxiv.org/abs/2402.03660)
- We discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/once_for_both.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression](https://arxiv.org/abs/2403.15835)

Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, **<u>Bo Zhang^(corr.)</u>**

[[**Project**]](https://github.com/HankYe/Once-for-Both)[[**Paper**]](https://arxiv.org/abs/2403.15835)
- We investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner.
- We present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for  Vision Transformer Compression (VTC) task.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='images/moe_framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2402.14800)

Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, **<u>Bo Zhang</u>**, Junchi Yan, Hongsheng Li

[[**Project**]](https://github.com/Lucky-Lance/Expert_Sparsity)[[**Paper**]](https://arxiv.org/abs/2402.14800)
- Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques.
- We present to post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLM.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/resimad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation](https://arxiv.org/abs/2309.05527)

**<u>Bo Zhang</u>**, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Xiangchao Yan, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2309.05527)
- Provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception scheme.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/uni3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection](https://arxiv.org/abs/2303.06880)

**<u>Bo Zhang</u>**, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2303.06880)
- Present a Uni3D which tackle multi-dataset 3D object detection from data-level and semantic-level.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/adpt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset](https://arxiv.org/abs/2306.00612)

Jiakang Yuan, **<u>Bo Zhang^(corr.)</u>**, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2306.00612)
- Build a large-scale pre-training point-cloud dataset with diverse data distribution, and meanwhile learn generalizable representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/bi3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection](https://arxiv.org/abs/2303.05886)

Jiakang Yuan, **<u>Bo Zhang^(corr.)</u>**, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2303.05886)
- Propose a Bi-domain active learning approach which select samples from both source and target domain to solve the cross-domain 3D object detection task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/regtta3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Reg-TTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection](https://link.springer.com/chapter/10.1007/978-3-031-72775-7_12)

Jiakang Yuan, **<u>Bo Zhang</u>**, Kaixiong Gong, Xiangyu Yue, Botian Shi, Yu Qiao, Tao Chen

[[**Project**]]()[[**Paper**]](https://link.springer.com/chapter/10.1007/978-3-031-72775-7_12)
- Explore a new task named test-time domain adaptive 3D object detection and propose a pseudo-label-based test-time adaptative 3D object detection method.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM'MM 2023</div><img src='images/sug.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification](https://arxiv.org/abs/2305.09160)

Siyuan Huang, **<u>Bo Zhang^(corr.)</u>**, Botian Shi, Peng Gao, Yikang Li, Hongsheng Li

[[**Project**]](https://github.com/SiyuanHuang95/SUG)[[**Paper**]](https://arxiv.org/abs/2305.09160)
- Propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. .
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI</div><img src='images/PAGCP.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Performance-aware Approximation of Global Channel Pruning for Multitask CNNs](https://arxiv.org/abs/2303.11923)

Hancheng Ye, **<u>Bo Zhang</u>**, Tao Chen, Jiayuan Fan, Bin Wang

[[**Project**]]()[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/9729102)
- We propose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first theoretically present the objective for achieving superior GCP, by considering the joint saliency of filters from intra- and inter-layers.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP</div><img src='images/sample_centric.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Sample-Centric Feature Generation for Semi-Supervised Few-Shot Learning](https://ieeexplore.ieee.org/abstract/document/9729102)

**<u>Bo Zhang</u>**, Hancheng Ye, Gang Yu, Bin Wang, Yike Wu, Jiayuan Fan, Tao Chen

[[**Project**]]()[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/9729102)
- Propose a sample-centric feature generation (SFG) approach for semi-supervised few-shot image classification.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM'MM 2022</div><img src='images/helixformer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2207.00784)

**<u>Bo Zhang</u>**, Jiakang Yuan, Baopu Li, Tao Chen, Jiayuan Fan, Botian Shi

[[**Project**]]()[[**Paper**]](https://arxiv.org/abs/2207.00784)
- Propose a Transformer-based double-helix model to achieve the cross-image object semantic relation mining in a bidirectional and symmetrical manner.
</div>
</div>

<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<div style='margin-top: 30pt'></div>



# üí¨ Invited Talks
- *2024.07*, Invited talk of **Multimodal Large Model Summit**. [[Video]](https://conf.csig.org.cn/6594/202407/44775.html)
- *2023.09*, Invited talk of **Effcient Pre-training of Autonomous Driving**. [[Video]](https://www.bilibili.com/video/BV1Sh4y1h7JK/?spm_id_from=333.337.search-card.all.click&vd_source=6efc9ee7fe0b74418e81b9b883e2a403)
- *2023.07*, Invited talk of **Towards 3D General Representation** at Techbeat. [[Video]](https://www.techbeat.net/talk-info?id=795)
- *2023.03*, Invited talk of **Transferable Pwerception of Autonomous Driving**. [[Video]](rom=333.337.search-card.all.click&vd_source=6efc9ee7fe0b74418e81b9b883e2a403)

<div style='margin-top: 30pt'></div>

# üíª Internships

- *2021.12 - 2022.06*, [Shanghai AI Laboratory](), China.

# üéì Ph.D Thesis

During his Ph.D. studies, Bo Zhang dedicated himself to advancing domain-adaptive models including 2D/3D domains. With a strong foundation in both theoretical research and practical applications, he has gained extensive expertise in model adaptation and continuous learning.

[[**Ph.D Thesis**]](https://drive.google.com/file/d/1CceNJTwc_QN6B3aE1rIscS65f4dYQdwF/view?usp=share_link)

<div style='margin-top: 30pt'></div>

# üìù Collaborators

- <a href="https://github.com/Alpha-Innovator">Alpha-Innovator</a>

<!-- - <a href="https://thinklab.sjtu.edu.cn/">ReThinkLab</a> -->
<!-- - <a href="http://leibai.site/">Lei Bai</a>
- <a href="https://gaopengcuhk.github.io/">Peng Gao</a>
- <a href="https://www.rjchen.site/">Runjian Chen</a>
- <a href="https://xyue.io/">Xiangyu Yue</a> -->
---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<div style='font-size:14pt; text-align:justify; font-family:Georgia; margin-top: 50pt'>
  <div style='width: 95%; vertical-align: middle; margin-left: 3%'>
Bo Zhang received the Ph.D. degree in electronic engineering from the School of Information Science and Technology, Fudan University. He is currently a Research Scientist in Shanghai AI Laboratory. His work has led to many awards, including Shanghai Rising Star under Grant No. 23QD1401000, awarded by the Shanghai Municipal Commission of Science and Technology, the National Scholarship 2021 China Award, the 2019 Excellent Doctoral Scholarship of Fudan University Award, and various awards from VALSE China and Shanghai Government. His research outcomes have some impacts on industrial applications like airport checkpoint security recognition, domain adaptive face recognition, and localization of concealed dangerous objects. 
  </div>
</div>

<div style='font-size:14pt; text-align:justify; font-family:Georgia; margin-top: 50pt'>
  <div style='width: 95%; vertical-align: middle; margin-left: 3%'>
He has published <b>30+ papers</b> in top-tier international conferences and journals such as CVPR, NeurIPS, ICML, ICLR, T-PAMI, TIP, T-MM, and IJCV. He also serves as a reviewer for several prestigious academic conferences and journals, including CVPR, ECCV, ICCV, ICLR, and ICML. He led the development of the 3DTrans general scene representation open-source project, which won the Waymo Challenge international competition and accumulated over 1.5k stars. Furthermore, he is committed to exploring the fundamental nature of long-chain reasoning in large models and aims to develop innovator-level agents through reinforcement learning methods and reflection mechanism.
  </div>
</div>

<div style='margin-top: 30pt'></div>

# 🔥 News
**2024:**
  - <p style='text-align:justify'><i>2024.12</i>: &nbsp;🎉🎉 Our research project, <a href="https://www.arxiv.org/abs/2412.11863/">GeoX</a>, has been officially open-sourced today. It is the first to explore formalized visual-language pre-training in enhancing geometric problem-solving abilities.</p>
  - <p style='text-align:justify'><i>2024.10</i>: &nbsp;🎉🎉 Grateful for the heartfelt recognition and thoughtful sharing of my research work <a href="https://www.sohu.com/a/814145951_121124291/">Fudan_CYL</a> and <a href="https://mp.weixin.qq.com/s/KVAI0Q7spu6yot-6oQOewQ">Fudan_SIST</a> .</p>
  - <p style='text-align:justify'><i>2024.10</i>: &nbsp;🎉🎉 The technical report for <a href="https://arxiv.org/abs/2409.18839/">MinerU</a>, an open-source solution for high-precision document content extraction, <font color="red"> has been published</font>. </p>
  - <p style='text-align:justify'><i>2024.09</i>: &nbsp;🎉🎉 Three papers accepted to <font color="red">NeurIPS 2024</font>: <a href="https://arxiv.org/abs/2410.09873/">AdaptiveDiffusion</a>, <a href="https://arxiv.org/abs/2411.05311/">ZOPP</a>, <a href="https://arxiv.org/abs/2405.15324/">LeapAD</a> </p>
  - <p style='text-align:justify'><i>2024.09</i>: &nbsp;🎉🎉 Previous evaluation metrics for Formula and Table Recognition tasks, such as BLEU and Edit Distrance, exhibit limitations. <a href="https://github.com/opendatalab/UniMERNet/tree/main/cdm/">Our CDM</a> has been released to ensure the evaluation objectivity by designing an image-level rather than LaTex-level metric score for Formula and Table Recognition evaluation.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp;🎉🎉 Bo Zhang was invited to serve as a PC member of <font color="red">AAAI 2025</font>.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp;🎉🎉 We open-sourced <font color="red">StructTable: Table Structural Extraction Model</font> <a href="https://huggingface.co/U4R/StructTable-base/">Models</a> and <a href="https://github.com/UniModal4Reasoning/StructEqTable-Deploy/">StructEqTable-Deploy</a>. It is a open-source repository to support the structuring tasks of visual tables.</p>
  - <p style='text-align:justify'><i>2024.08</i>: &nbsp;🎉🎉 We collaborated with the OpenDataLab team to open-source the <font color="red">PDF-Extract-Kit</font>. It can extract high-quality and structured content from PDFs and has gained <b>6K+</b> stars.</p>
  - <p style='text-align:justify'><i>2024.07</i>: &nbsp;🎉🎉 One paper (Reg-TTA3D) is accepted by <font color="red">ECCV 2024</font>. We explore test-time adaptive 3d object detection for the first time.</p>
  - <p style='text-align:justify'><i>2024.03</i>: &nbsp;🎉🎉 One paper is accepted by <font color="red">ACL 2024</font>. We propose All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.</p>
  - <p style='text-align:justify'><i>2024.02</i>: &nbsp;🎉🎉 One paper (Once for Both) is accepted by <font color="red">CVPR 2024</font>. Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression.</p>
  - <p style='text-align:justify'><i>2024.01</i>: &nbsp;🎉🎉 One paper (ReSimAD) is accepted by <font color="red">ICLR 2024</font>. We propose a zero-shot generalization framework by reconstructing mesh and simulating target point clouds.</p>
  - <p style='text-align:justify'><i>2024.01</i>: &nbsp;🎉🎉 Two papers (<a href="https://ieeexplore.ieee.org/abstract/document/10516600/">IPNet</a> and <a href="https://ieeexplore.ieee.org/abstract/document/10360874/">MVNet</a>) are accepted by <font color="red">TCSVT</font>. </p>

**2023:**
  - <p style='text-align:justify'><i>2023.12</i>: &nbsp;🎉🎉 We have released the <a href="https://github.com/UniModal4Reasoning/ChartVLM?tab=readme-ov-file">ChartX benchmark</a>, covering 18 chart types, 7 chart tasks, 22 disciplinary topics to evaluate the chart-related capabilities of the existing MLLMS.</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp;🎉🎉 StructChart: our research on visual chart, has been released <a href="https://arxiv.org/abs/2309.11268">arXiv paper</a>, where we will release the SimChart9K dataset powered by LLM. By the proposed SimChart9K, we observe that StructChart continuously improves the chart perception performance as more simulated charts are used for pre-training.</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp;🎉🎉 SPOT, showing a promising and <b>scalable</b> 3D pre-training on autonomous driving, has been released (See our paper for more details, <a href="https://arxiv.org/abs/2309.10527">arXiv paper</a>).</p>
  - <p style='text-align:justify'><i>2023.09</i>: &nbsp;🎉🎉 - One paper entitled “AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset” is accepted by <font color="red">NeurIPS-2023</font>.</p>
  - <p style='text-align:justify'><i>2023.07</i>: &nbsp;🎉🎉 One paper about cross-domain background-fouced alignment "Rethinking Cross-Domain Pedestrian Detection: A Background-Focused Distribution Alignment Framework for Instance-Free One-Stage Detectors" is accepted by <font color="red">TIP</font>.</p>
  - <p style='text-align:justify'><i>2023.07</i>: &nbsp;🎉🎉 One paper entitled "SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification" is accepted by <font color="red">ACM MM-2023</font>.</p>
  - <p style='text-align:justify'><i>2023.04</i>: &nbsp;🎉🎉 One paper entitled "Performance-aware Approximation of Global Channel Pruning for Multitask CNNs" is accepted for publication in <font color="red">T-PAMI</font>.</p>
  - <p style='text-align:justify'><i>2023.03</i>: &nbsp;🎉🎉 <b>Three papers</b> are accepted by <font color="red">CVPR-2023</font>: Uni3D, Bi3D, GDP.</p>
  - <p style='text-align:justify'><i>2023.02</i>: &nbsp;🎉🎉 Bo Zhang started to work on exploring how to improve the problem-solving and reasoning ability of LLMs or VLMs for complicated modalities, including <b>Chart, Table, Geometry, Scientific Document</b>, by investigating foundation LLM models from the perspective of structured knowledge-rich data.</p>


<div style='margin-top: 30pt'></div>

# 📝 Selected Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SCIS</div><img src='images/internvl-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)

Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, **<u>Bo Zhang</u>**, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang

[[**Project**]](https://github.com/OpenGVLab/InternVL)[[**Paper**]](https://arxiv.org/abs/2404.16821)
- Propose InternVL 1.5 and InternVL 2. (<font color="red">Rank 1st</font> among open-source VLM models on MMMU, DocVQA, ChartQA, and MathVista.) 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/adaptivediffusion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy](https://arxiv.org/abs/2410.09873)

Hancheng Ye, Jiakang Yuan, Renqiu Xia, Xiangchao Yan, Tao Chen, Junchi Yan, Botian Shi, **<u>Bo Zhang^(corr.)</u>**

[[**Project**]](https://jiakangyuan.github.io/AdaptiveDiffusion-project-page/)[[**Paper**]](https://arxiv.org/abs/2410.09873)
- Propose AdaptiveDiffusion to adaptively reduce the noise prediction steps during the denoising proces guided by the third-order latent difference. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/zopp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ZOPP: A Framework of Zero-shot Offboard Panoptic Perception for Autonomous Driving](https://arxiv.org/abs/2411.05311)

Tao Ma, Hongbin Zhou, Qiusheng Huang, Xuemeng Yang, Jianfei Guo, **<u>Bo Zhang</u>**, Min Dou, Yu Qiao, Botian Shi, Hongsheng Li

[[**Project**]]()[[**Paper**]](https://arxiv.org/abs/2411.05311)
- ZOPP integrates the powerful zero-shot recognition capabilities of vision foundation models and 3D representations derived from point clouds.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/leapad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving](https://arxiv.org/abs/2405.15324)

Jianbiao Mei, Yukai Ma, Xuemeng Yang, Licheng Wen, Xinyu Cai, Xin Li, Daocheng Fu, **<u>Bo Zhang</u>**, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yong Liu, Yu Qiao

[[**Project**]](https://leapad-2024.github.io/LeapAD/)[[**Paper**]](https://arxiv.org/abs/2405.15324)
- LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/ctl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm](https://arxiv.org/abs/2402.03660)

Zhanpeng Zhou, Zijun Chen, Yilan Chen, **<u>Bo Zhang^(corr.)</u>**, Junchi Yan

[[**Project**]](https://github.com/zzp1012/Cross-Task-Linearity)[[**Paper**]](https://arxiv.org/abs/2402.03660)
- We discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/once_for_both.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression](https://arxiv.org/abs/2403.15835)

Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, **<u>Bo Zhang^(corr.)</u>**

[[**Project**]](https://github.com/HankYe/Once-for-Both)[[**Paper**]](https://arxiv.org/abs/2403.15835)
- We investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner.
- We present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for  Vision Transformer Compression (VTC) task.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/resimad.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation](https://arxiv.org/abs/2309.05527)

**<u>Bo Zhang</u>**, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Xiangchao Yan, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2309.05527)
- Provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception scheme.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/uni3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection](https://arxiv.org/abs/2303.06880)

**<u>Bo Zhang</u>**, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2303.06880)
- Present a Uni3D which tackle multi-dataset 3D object detection from data-level and semantic-level.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/adpt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset](https://arxiv.org/abs/2306.00612)

Jiakang Yuan, **<u>Bo Zhang^(corr.)</u>**, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2306.00612)
- Build a large-scale pre-training point-cloud dataset with diverse data distribution, and meanwhile learn generalizable representations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/bi3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection](https://arxiv.org/abs/2303.05886)

Jiakang Yuan, **<u>Bo Zhang^(corr.)</u>**, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao

[[**Project**]](https://github.com/PJLab-ADG/3DTrans)[[**Paper**]](https://arxiv.org/abs/2303.05886)
- Propose a Bi-domain active learning approach which select samples from both source and target domain to solve the cross-domain 3D object detection task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/regtta3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Reg-TTA3D: Better Regression Makes Better Test-time Adaptive 3D Object Detection](https://link.springer.com/chapter/10.1007/978-3-031-72775-7_12)

Jiakang Yuan, **<u>Bo Zhang</u>**, Kaixiong Gong, Xiangyu Yue, Botian Shi, Yu Qiao, Tao Chen

[[**Project**]]()[[**Paper**]](https://link.springer.com/chapter/10.1007/978-3-031-72775-7_12)
- Explore a new task named test-time domain adaptive 3D object detection and propose a pseudo-label-based test-time adaptative 3D object detection method.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM'MM 2023</div><img src='images/sug.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification](https://arxiv.org/abs/2305.09160)

Siyuan Huang, **<u>Bo Zhang^(corr.)</u>**, Botian Shi, Peng Gao, Yikang Li, Hongsheng Li

[[**Project**]](https://github.com/SiyuanHuang95/SUG)[[**Paper**]](https://arxiv.org/abs/2305.09160)
- Propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. .
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI</div><img src='images/PAGCP.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Performance-aware Approximation of Global Channel Pruning for Multitask CNNs](https://arxiv.org/abs/2303.11923)

Hancheng Ye, **<u>Bo Zhang</u>**, Tao Chen, Jiayuan Fan, Bin Wang

[[**Project**]]()[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/9729102)
- We propose a Performance-Aware Global Channel Pruning (PAGCP) framework. We first theoretically present the objective for achieving superior GCP, by considering the joint saliency of filters from intra- and inter-layers.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP</div><img src='images/sample_centric.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Sample-Centric Feature Generation for Semi-Supervised Few-Shot Learning](https://ieeexplore.ieee.org/abstract/document/9729102)

**<u>Bo Zhang</u>**, Hancheng Ye, Gang Yu, Bin Wang, Yike Wu, Jiayuan Fan, Tao Chen

[[**Project**]]()[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/9729102)
- Propose a sample-centric feature generation (SFG) approach for semi-supervised few-shot image classification.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM'MM 2022</div><img src='images/helixformer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2207.00784)

**<u>Bo Zhang</u>**, Jiakang Yuan, Baopu Li, Tao Chen, Jiayuan Fan, Botian Shi

[[**Project**]]()[[**Paper**]](https://arxiv.org/abs/2207.00784)
- Propose a Transformer-based double-helix model to achieve the cross-image object semantic relation mining in a bidirectional and symmetrical manner.
</div>
</div>

<!-- # 🎖 Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<div style='margin-top: 30pt'></div>



# 💬 Invited Talks
- *2024.07*, Invited talk of **Multimodal Large Model Summit**. [[Video]](https://conf.csig.org.cn/6594/202407/44775.html)
- *2023.09*, Invited talk of **Effcient Pre-training of Autonomous Driving**. [[Video]](https://www.bilibili.com/video/BV1Sh4y1h7JK/?spm_id_from=333.337.search-card.all.click&vd_source=6efc9ee7fe0b74418e81b9b883e2a403)
- *2023.07*, Invited talk of **Towards 3D General Representation** at Techbeat. [[Video]](https://www.techbeat.net/talk-info?id=795)
- *2023.03*, Invited talk of **Transferable Pwerception of Autonomous Driving**. [[Video]](rom=333.337.search-card.all.click&vd_source=6efc9ee7fe0b74418e81b9b883e2a403)

<div style='margin-top: 30pt'></div>

# 💻 Internships

- *2021.12 - 2022.06*, [Shanghai AI Laboratory](), China.

<div style='margin-top: 30pt'></div>

# 📝 Collaborators

- <a href="https://thinklab.sjtu.edu.cn/">ReThinkLab</a>
- <a href="http://leibai.site/">Lei Bai</a>
- <a href="https://gaopengcuhk.github.io/">Peng Gao</a>
- <a href="https://www.rjchen.site/">Runjian Chen</a>
- <a href="https://xyue.io/">Xiangyu Yue</a>